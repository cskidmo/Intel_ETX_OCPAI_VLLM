= Inference Server on Multiple Platforms

== Existing lab resources

. RH Inference server on multiple platforms +
https://github.com/redhat-ai-services/inference-service-on-multiple-platforms[^]

. RH Inference server tutorial +
https://docs.google.com/document/d/11-Oiomiih78dBjIfClISSQBKqb0Ij4UJg31g0dO5XIc/edit?usp=sharing[^]

== Potential Topics to Cover in the Lab

[#rhel]
=== RHEL
This section will walk you through deploying RH Inference server on RHEL.

== Host Verification

Before proceeding, it is critical to verify that the host environment is correctly configured.
Check Driver Status: After the system reboots, run the nvidia-smi (NVIDIA System Management Interface) command. A successful configuration will display a table listing all detected NVIDIA GPUs, their driver versions, and CUDA versions.14

```
nvidia-smi
```

image::nvidia-smi-screenshot.png[nvidia-smi-screenshot.png]

== Install nvidia-container-toolkit
The NVIDIA Container Toolkit is the crucial bridge that allows container runtimes like Podman or Docker to securely access the host's GPUs.

```
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
sudo dnf-config-manager --enable nvidia-container-toolkit-experimental
sudo dnf install -y nvidia-container-toolkit
```

== Configure CDI

```
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
# check the config
nvidia-ctk cdi list
```

image::cdi-list.png[cdi-list.png]

Test Container-GPU Access: To confirm that Podman can access the GPUs, run a simple test workload using a standard NVIDIA CUDA sample image. This step definitively validates the entire stack, from the driver to the container runtime.

```
sudo podman run --rm --device nvidia.com/gpu=all nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubi8
podman run --rm -it \
--security-opt=label=disable \
--device nvidia.com/gpu=all \
nvcr.io/nvidia/cuda:12.4.1-base-ubi9 \
nvidia-smi
```

image::gpu-passthrough-test.png[gpu-passthrough-test.png]


== Logging Into Red Hat Container Registry
Login to registry.redhat.io

[source,sh,role=execute]
----
sudo podman login registry.redhat.io
----

== Running vLLM on RHEL
Clone the repository with RH Inference server for RHEL 

[source,sh,role=execute]
----
git clone https://github.com/redhat-ai-services/etx-llm-optimization-and-inference-leveraging.git
----

Run the vllm pod


[source,sh,role=execute]
----
sudo podman kube play etx-llm-optimization-and-inference-leveraging/optimization_lab/rhel/vllm-pod.yaml
----

Open a new terminal to follow the logs.

[source,sh,role=execute]
----
sudo podman logs --follow vllm-vllm 
----

List all of the models deployed on the RH Inference server.

[source,sh,role=execute]
----
curl http://127.0.0.1:80/v1/models
----

Go back to your original terminal and send in a request to the model endpoint.
[source,sh,role=execute]
----
curl -X POST -H "Content-Type: application/json" -d '{
    "prompt": "What is the capital of France?",
    "max_tokens": 100
}' http://127.0.0.1:80/v1/completions | jq
----

Go to your other terminal and view the logs. You should see a successful log entry. 

image::successful-request-to-vllm.png[successful-request-to-vllm.png]

== Troubleshooting
If you need to remove and cleanup vllm pod

[source,sh,role=execute]
----
sudo podman pod stop vllm && sudo podman pod rm vllm
----

[#ocp]
=== OpenShift

[#ubuntu]
=== Ubuntu
