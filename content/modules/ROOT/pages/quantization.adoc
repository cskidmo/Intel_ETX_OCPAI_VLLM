# Quantization: Why 

Model quantization emerges as a pivotal technique to address these challenges. 
It involves reducing the numerical precision of LLM parameters, such as weights and activations, 
from higher-precision floating-point formats (e.g., FP32, FP16, BF16) to lower-bit representations (e.g., INT8, INT4, FP8, FP6, FP4).

This process fundamentally decreases the model's memory footprint, accelerates inference speeds, and reduces overall computational load 
and energy costs (*depending on accelerator support).

Consider:

Llama Maverick 400B

| Optimization | Params Size (GB) | GPUs |
|---|---|---|
| BFloat16 | 400 * 2 ~= 800GB | 10 x 80GB ← requires multi-node! |
| INT8/FP8 | 400 * 1 ~= 400GB | 5 x 80GB |
| INT4 | 400 * 0.5 ~= 200GB | 3 x 80GB |


Reducing the memory occupied by the model means there is more memory to allocate towards KV Cache, which will result in end-to-end 
faster serving on cache hits.
Furthermore, lowering the precision of weights results in reduced data movement from HBM to Shared memory and registers on device.

An additional beneficial property is that _some_ accelerators will be able to compute the lower precision values at higher speed: think integer math
is inherently cheaper to compute than floating point, therefore the device will be able to leverage the integer-only hardware to run faster. 

The fundamental principle of quantization is to represent numerical values, including both model weights and intermediate activations,
using a reduced number of bits. 
At its core, quantization maps a range of floating-point values (denoted as X) to a subset of continuous or even discrete 
(integers) values (X̄) represented with a smaller number of bits. This transformation is typically defined by a scaling factor (Δ), a rounding function 
and optionally a zero-point (z).
 
```
 X̄ = ⌈X*Δ+z⌋. 
```

# Quantization: What 

A naive quantization range mapping from bf16 (16bits) down to fp8 (8 bits) can be visualized as:

image::quant_mapping.png[quant_mapping]

The issue with just mapping values this way is that given how the two types will have different granularities, 
much of the values which don't fall into the representable grid in fp8 will be lost (ie they will be mapped to the same value, *clipped*).
This clipping or loss of precision can lead to a significant degradation in model accuracy. 

The workaround to this fundamental representational issue is to adjust the mapping range based on the actual distribution of values we are quantizing. 
Instead of a fixed, global mapping, we determine the optimal scaling factor (Δ) and zero-point (z) by first understanding the spread 
of the floating-point values (X) within a specific weight tensor.

This process typically involves the following steps:

 - Observe the Value Distribution: For a given tensor, we identify the minimum (X min) and maximum (X max) values present in that tensor.
   Example: If a tensor's values range from -5.2 to +4.8, these become our observed min and max.
 - Determine the Quantized Range: Next, we need to know the numerical range that our target lower-precision format. 
    For an 8-bit integer, for instance, this might be from -128 to +127 (signed int8).
    Calculate the Scaling Factor (Δ): The scaling factor bridges the gap between the floating-point range and the target range. 
    It tells us how many floating-point units correspond to one step in the quantized space.
​
 - Optionally calculate the Zero-Point (z): The zero-point is an offset that aligns the zero value of the floating-point range with a
  specific value in the quantized range. This is called _asymmetric_ quantization where the floating-point distribution is not centered 
  around zero.

​ - Quantize the Values: Once Δ and z are determined, each original floating-point value X can be mapped to its quantized representation X̄ `X̄ = ⌈X*Δ+z⌋`.
    The round function ensures that floating-point values are mapped to the nearest representable integer. Values that fall outside the observed min, max
    range will be "clipped".


image::quant_mapping2.png[quant_mapping2]


# Weight vs activations quant

- Weights: These are the learned parameters of the model. They are static values determined during the training phase, 
representing the "knowledge" the model has acquired. Once training is complete, these values don't change during inference.

- Activations: These are the intermediate outputs generated by the model as it processes an input. As data flows through the network, 
each layer performs calculations using the weights and the preceding layer's activations to produce new activations. 
These values are dynamic; they change with every new input the model receives.

Activations, being dynamic, require data. Their values depend entirely on the specific input provided to the model. 
To accurately determine the optimal Δ and z for activations, you need to observe how they behave across a range of typical inputs.
This is usually done through a process called calibration, where a small, representative dataset (distinct from the training or validation sets) is passed through the full-precision model to collect statistics (like min/max values) for each activation tensor. Without this data, the model wouldn't know the proper range to map the dynamic activation values into the lower-precision format.



References:
 - vLLM Office hours #23 