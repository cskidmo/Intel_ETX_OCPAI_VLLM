= vLLM & Performance Tuning

== Case Study: Latency Optimization for granite-3.1-8b-instruct with vLLM

Let's apply our knowledge to a concrete scenario: you need to serve granite-3.1-8b-instruct, and your primary goal is to minimize inference latency pass:[<span title="Latency: The time it takes for a single request to complete." style="cursor: help;">&#128161;</span>] for a given number of concurrent users -32 in this case- with an expected generation length <2048 tokens. 
As you have limited flexibility to scale GPU resources, you want to understand the key vLLM parameters and strategies to maximize performance before simply adding more hardware.

== Introduction

Before diving into tuning, it's important to understand the key performance considerations in vLLM:

*Latency*: The time it takes for a single request to complete. Of particular importance here is going to be tracking the Time To First Token - *TTFT* pass:[<span title="TTFT: How quickly the user sees the first word of the response." style="cursor: help;">&#128161;</span>], which will ensure the user can experience snappier responses.

*Throughput*: The number of requests (or tokens) processed per unit of time. While we're optimizing for latency, higher throughput often means better resource utilization, which can indirectly help avoid latency spikes due to queuing.

. First we'll need to deploy the granite-3.1-8b-instruct. Open a terminal and login to your OpenShift cluster. Create a new project name rhaiis-demo.
+
[source,sh,role=execute]
----
oc new-project rhaiis-demo
----

. We'll be deploying granite-3.1-8b-instruct through a Helm chart. Clone the following repo:
+
[source,sh,role=execute]
----
git clone https://github.com/redhat-ai-services/inference-service-on-multiple-platforms.git
----

. Open the _openshift-ai/values.yaml_ file so we can add our engine arguments and specify the storageUri. Your file should look similar to the one below.
+
[source,sh,role=execute]
----
inferenceService:
  storage:
    mode: uri
    storageUri: oci://image-registry.openshift-image-registry.svc:5000/rhaiis-demo/granite-8b-instruct-vllm-kserve
  args:
    - "--disable-log-requests"
    - "--max-num-seqs=32"
    - "--max-model-len=2048"
----

*--disable-log-requests* is used to prevent the logging of detailed request information. By default, vLLM logs various details about each incoming request, which can be useful for debugging and monitoring. However, in production environments or when high performance is critical, excessive logging can consume resources and generate large log files. The --disable-log-requests flag addresses this by turning off the detailed logging of individual requests.

*--max-num-seqs* configuring the server to handle a maximum of 32 concurrent requests in a single parallel computation step (a batch). For example, if 32 different users send a prompt to your chatbot at the same time.

*--max-model-len* argument sets the maximum number of tokens that the vLLM server will allow for a single sequence. This total includes both the user's prompt and the generated response combined.

. Now that we've set the engine arguments we'll go ahead and deploy the granite model with Helm.
+
[source,sh,role=execute]
----
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
helm upgrade -i granite-8b-instruct redhat-ai-services/vllm-kserve --version "0.3.8" \
  --values openshift-ai/values.yaml
----

. Login to OpenShift AI and go to your *rhaiis-demo* Data Science Project. Wait until the model fully deploys (green check) before continuing. 

image::granite-deployed-rhoai.png[Granite deployed on RHOAI]

. We'll be running an OpenShift pipeline with GuideLLM to benchmark our model as we step through the optimizations.


//// 
We can use this script as our starting point and the way we're going to benchmark our model as we step through the optimizations:

```bash
#!/bin/bash

MODEL=ibm-granite/granite-3.1-8b-instruct
LOG_PREFIX=

MAX_NUM_SEQS=32
PORT=8000
HEALTH_ENDPOINT="http://localhost:$PORT/health"
DEVICES="0"
REQUEST_RATES="1 4 8 16"

VLLM_CMD="CUDA_VISIBLE_DEVICES=$DEVICES vllm serve $MODEL --disable-log-requests --port $PORT --max-num-seqs $MAX_NUM_SEQS --max-model-len 2048 &"

# Function to clean up if script is interrupted
cleanup() {
    echo "Stopping vLLM (PID=$VLLM_PID)..."
    kill "$VLLM_PID" 2>/dev/null || true
    wait "$VLLM_PID" 2>/dev/null || true
}
trap cleanup EXIT


start_vllm() {
    eval $VLLM_CMD
    VLLM_PID=$!

    # Wait for /health endpoint to be ready
    echo "Waiting for vLLM to become healthy..."
    until curl -sf "$HEALTH_ENDPOINT"; do
        if ! ps -p $VLLM_PID > /dev/null; then
            echo "vLLM process exited unexpectedly."
            exit 1
        fi
        sleep 2
    done

    echo "vLLM is up and healthy!"
}

for request_rate in $REQUEST_RATES; do
    BM_LOG="bm_${LOG_PREFIX}_${request_rate}.log"
    echo "Running benchmark $BM_LOG"
    # Start vLLM from scratch to avoid prefix cache interaction across request_rates (worst-case measurements)
    start_vllm
    vllm bench serve \
        --backend vllm \
        --model $MODEL \
        --dataset-name random \
        --random-input-len 800 \
        --random-output-len 128 \
        --request-rate $request_rate \
        --ignore-eos \
        --num-prompts 100 \
        --port $PORT | tee "$BM_LOG"   
    # Stop vLLM
    cleanup
done

```
The script will simply spin up a vLLM instance and benchmark for a particular amount of concurrent users.
To benchmark the model here, we're going to simulate an artificial "dataset" using "vllm bench" utility command.
////


Here's the starting results on single NVIDIA L4 GPU at vllm (`d0dc4cfca`), focusing on TTFT:

image::starting_point.png[starting_point]

All reported times are in `ms`. All reported times are in milliseconds. Notice how quickly we exceed the "seconds" threshold with even a 
slight increase in concurrent usersâ€”serving LLMs is hard!  


# vLLM Tuning Strategies for Llama 3 8B Latency

Granite-3.1-8b-instruct is a popular, powerful small-size _dense_ model. 

Here are the primary avenues for optimization:

## GPU Allocation & Batching Parameters: Managing Concurrency

For a "given amount of concurrent users," how you manage batching is critical to maximize GPU utilization without introducing excessive queueing latency.
Let's take a look at some of the most popular vllm configurations.

`--max-model-len`: The maximum sequence length (prompt + generated tokens) the model can handle.
Goal: Set this to the minimum _reasonable_ length for your use case. Too small means requests get truncated; too large means less space for KVCache, which will impact your performance.
At startup, vllm will profile the model using this value, as it needs to ensure it is able to serve at least one request with length=max-model-len.
This is also a trade-off with the next parameter, `max-num-seqs`.
Tuning: If most of your requests are short, keeping max-model-len tighter can allow more requests into the batch (by increasing `max-num-seqs`).

NOTE: `max-num-batched-tokens` is a highly related parameter. It's limiting the amount of tokens the scheduler can schedule, rather than what the model can produce.
So the actual number limiting the amount of memory allocated for the model runtime is actually `min(max-model-len, max-num-batched-tokens)`.

You can verify the impact of this parameter by increasing its value when starting vLLM and then observing the amount of memory reserved for KVCache.
Check out the logs for our starting config:
```
# vllm serve ibm-granite/granite-3.3-8b-instruct --disable-log-requests --max-num-seqs 32 --max-model-len 2048
. . .
INFO 07-17 08:33:11 [gpu_worker.py:244] Available KV cache memory: 4.22 GiB
INFO 07-17 08:33:11 [kv_cache_utils.py:732] GPU KV cache size: 27,680 tokens
```

And then try to increase model size to something like `--max-model-len 4096 --max-num-batched-tokens 4096`:
```
INFO 07-17 08:57:54 [gpu_worker.py:244] Available KV cache memory: 4.06 GiB
INFO 07-17 08:57:55 [kv_cache_utils.py:732] GPU KV cache size: 26,608 tokens
```


`--max-num-seqs`: The maximum number of sequences (requests) that can be processed concurrently. This is often referred to as the batch size, allowing for higher throughput.
Goal: Set this to the minimum _reasonable_ length for your use case. When this is too high, your requests under load might get fractioned into smaller 
chunks resulting in higher end-to-end latency. If this too low, you might be under-utilizing your GPU resources.
Let's see this case in practice. Modify the script to limit the number max requests to 1 and run with 4 requests at a time.
```
LOG_PREFIX=batch1
MAX_NUM_SEQS=1
REQUEST_RATES=4
```
What is happening here is that the engine is effectively being throttled and is only executing one request at a time. 
```
# Batch32 results
---------------Time to First Token----------------
Mean TTFT (ms):                          13283.81  
Median TTFT (ms):                        11838.76  
P99 TTFT (ms):                           35857.18  

# Batch4 results
---------------Time to First Token----------------
Mean TTFT (ms):                          104897.19 
Median TTFT (ms):                        104294.83 
P99 TTFT (ms):                           211148.64 

```
This is almost 10x slower!

Also notice another important indicator of an unhealthy deployment from the logs:
```
Engine 000: Avg prompt throughput: 80.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs,==>Waiting: 95 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 16.3%
```
Especially when coupled with high waiting time (`vllm:request_queue_time_seconds` metric from `/v1/metrics`).


## Model Quantization

Quantization is arguably the most impactful change you can make for latency, especially with vLLM's efficient kernel implementation for w8a16 or w4a16.

Why? Reducing precision directly shrinks the model's memory footprint and enables faster arithmetic on modern GPUs.

What to try (_highly_ dependent on available hardware):

FP8: If you have access to NVIDIA H100 GPUs or newer (e.g., B200), FP8 (E4M3 or E5M2) is a game-changer. These GPUs have dedicated FP8 Tensor Cores that 
offer significantly higher throughput compared to FP16. This provides a direct path to lower latency per token without significant accuracy loss 
for Llama 3 models.

INT8 (e.g., AWQ): Starting with A100 or even A6000/3090 GPUs, INT8 quantization is an excellent choice. It reduces the model to 8B * 1 byte = 8GB, 
halving the memory footprint and enabling faster integer operations. 

INT4: If you're pushing for absolute minimum latency and can tolerate a small accuracy trade-off, INT4 (e.g., via AWQ or other 4-bit methods) 
can reduce the model to 8B * 0.5 bytes = 4 GB. This is extremely memory-efficient and, on some hardware, can offer further speedups. 
Test accuracy thoroughly with your specific use case, as 4-bit can sometimes be more sensitive.
Similarly, check out FP4 versions when Nvidia Blackwell hardware is available.

```
| Quantization Type | Recommended Hardware             | Key Benefits for Latency                                            | Memory Footprint (for Llama 3 8B) | Accuracy Consideration                                            | Notes                                                                                                                                                                                                            |
| :---------------- | :------------------------------- | :------------------------------------------------------------------ | :-------------------------------- | :---------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **FP8 (E4M3/E5M2)** | NVIDIA H100 (or newer)     | - Dedicated FP8 Tensor Cores for significantly higher throughput.   | 8B * 1 byte ~= 8 GB               | Minimal accuracy loss for Llama 3 models.                         | Already a standard for high-performance inference.                                                                                                                                                         |
| **INT8 (e.g., AWQ)** | NVIDIA A100, A6000 (or newer) | - Halves memory footprint.                                          | 8B * 1 byte ~= 8 GB               | Generally decent accuracy preservation.                           | Widely supported (across manufacturers) and fast.                                                                                                                                                        |
| **INT4 (e.g., AWQ)** | NVIDIA A100, A6000 (or newer) | - Extremely memory-efficient.                                       | 8B * 0.5 bytes ~= 4 GB            | Requires an accuracy trade-off.                                   | Pushes for absolute minimum latency.                                                                                                                                                                     |
| **FP4** | NVIDIA Blackwell (B200)          | - New architecture support for even lower-precision floating-point. | 8B * 0.5 bytes ~= 4 GB            | Designed to maintain better accuracy than integer 4-bit, but still requires validation. | Emerging standard with the latest hardware (e.g., NVIDIA Blackwell). Look for NVFP4 variants.                                                                                                            |
```

Please refer to the compatiblity chart https://docs.vllm.ai/en/latest/features/quantization/supported_hardware.html for up to date quantization support in vLLM.

Let us try to run a w8a8 int8 model, by referring to the original script:
```
MODEL=RedHatAI/granite-3.1-8b-instruct-quantized.w8a8
LOG_PREFIX=int8
```

This is what we get, focusing on TTFT:

image::quant_vs_unquant.png[quant_vs_unquantized]

Up to 2x speedup!


## Using a smaller model 

Following the same principle as quantization, serving a smaller model (when accuracy on task is acceptable) will enable faster response
times as less data is moved around (model weights+activations) and less sequential computations are involved (generally fewer layers).
For this particular use-case, consider `ibm-granite/granite-3.1-2b-instruct`.


### Using a different model

While Granite 3 is a strong dense model, for certain latency-sensitive scenarios, considering a Mixture-of-Experts (MoE) model like Mixtral 8x7B could be a 
compelling alternative.

Why MoE for Latency? MoE models have a large total number of parameters (e.g., Mixtral 8x7B has 47B total parameters), but critically, 
they only activate a sparse subset of these parameters (e.g., 13B for Mixtral 8x7B) for each token generated. 
This means the actual computational cost per token is significantly lower than a dense model of its total parameter count.
Which is especially true when sharding experts over multiple GPUs with MoE especially with vLLM's optimized handling of MoE sparsity. 

Trade-offs: While MoE models can offer lower inference latency per token due to their sparse activation, they still require enough GPU memory 
to load the entire model's parameters, not just the active ones. So, Mixtral 8x7B will demand more VRAM than Llama 3 8B,
even if it's faster per token. You'll need sufficient GPU memory (e.g., a single A100 80GB or multiple smaller GPUs with tensor parallelism) to fit the full 47B parameters.

vLLM has strong support for MoE models like Mixtral, including optimizations for their unique sparse compute patterns and dynamic routing.

Consider When: Your application might benefit from the increased quality often associated with larger (total parameter) MoE models, combined with the per-token speed advantages 
of their sparse computation.


## Speculative Decoding.

Speculative decoding is a powerful technique to reduce the generation latency, particularly noticeable for the Time To First Token (TTFT).
Speculative decoding is fundamentally a tradeoff: spend a little bit of extra compute to reduce memory movement.
The extra compute is allocated towards the smaller draft model and consequent proposer verifying step.
At low request rates, we are memory-bound, so reducing memory movement can really help with latency. 
However, at higher throughputs or batch sizes, we are compute-bound, and speculative decoding can provide worse performance. 

image::spec_dec.png[spec_dec]

The graph here from https://developers.redhat.com/articles/2025/07/01/fly-eagle3-fly-faster-inference-vllm-speculative-decoding#speculative_decoding__a_solution_for_faster_llms
highlighs the tradeoffs of speculative decoding at low request rate vs bigger batch sizes.
Take away message: as long as the number of requests is bound to use a non-intensive amount of GPU resources (lower req/s), spec decoding can provide
a nice speedup.

NOTE: Speculative decoding in vLLM is not yet fully optimized and does not always yield intended inter-token latency reductions. In particular in this case it will fallback to V0 due to
V1 still not supporting this particular speculation technique. Mind that what we're comparing here is not going to be exactly apples to apples, as the V0 and V1 engine have quite
substantial architectural differences. 

What to try: You'll need to specify a smaller draft model. A good starting point for Llama/granite might be a smaller Llama/granite variant or as in this 
example a speculator trained specifically for our use-case. Let's change the vllm startup command:

```bash
VLLM_CMD="vllm serve $MODEL --max-num-seqs $MAX_NUM_SEQS --max-model-len 2048 --enable-chunked-prefill --max-num-batched-tokens 2048  --speculative-config\
 '{\"model\": \"ibm-granite/granite-3.0-8b-instruct-accelerator\", \"num_speculative_tokens\": 4, \"draft_tensor_parallel_size\": 1}' &"
```

vLLM will spin up an instance with the two models. 
There's no free-lunch though, mind that the GPU memory will now be comprised of: the original `ibm-granite/granite-3.1-8b-instruct` weights + `ibm-granite/granite-3.0-8b-instruct-accelerator` proposer weights
 + a KV cache for *both* models.

image::spec.png[specized]


A key metric to keep an eye on when serving a speculator is the `acceptance rate`:
```
INFO 07-17 11:11:38 [metrics.py:439] Speculative metrics: Draft acceptance rate: 0.381, System efficiency: 0.427, Number of speculative tokens: 3, Number of accepted tokens: 3781, Number of draft tokens: 9930, Number of emitted tokens: 5657.
```

This is the percentage of tokens being produced by the speculator that match the ones of the draft model.
Here we're still on the lower side as ideally you would want to see this number be higher.

This is tied to major drawback holding back the adoptability of speculative decoding, which is the fact that the speculator needs to be trained specifically for the model you intend to deploy,
in order to achieve an high acceptance rate.
Being a data-dependent technique, this is mostly useful when it is 


### Final Notes

Optimization is an iterative process. As you tune vLLM, continuously monitor key metrics:

- Time To First Token (TTFT): Critical for interactive applications.
- Throughput (Tokens/sec or Requests/sec): To ensure your concurrency goals are met.
- GPU Utilization: High utilization indicates efficient use of resources.
- GPU KV cache usage: At very high rates early on into a benchmark, it is an indicator of likely insufficient memory for KV cache.

== Existing Slides

. PSAP LLM Performance Benchmarking - July 11 2025 +
https://docs.google.com/presentation/d/1IXReNsWRUcy1C9nGsnnhkG_H-OG5UQ2nYS2KmrXr340/edit?usp=sharing[^]

== Existing lab resources

. Training: vLLM Master Class: +
https://redhat-ai-services.github.io/vllm-showroom/modules/index.html[^]

. Training: Optimizing vLLM for RHEL AI and OpenShift AI: +
https://rhpds.github.io/showroom-summit2025-lb2959-neural-magic/modules/index.html[^]

. RH Inference server docs - key vLLM serving arguments +
https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.1/html-single/vllm_server_arguments/index#key-server-arguments-server-arguments

. vLLM: Optimizing and Serving Models on OpenShift AI
https://redhatquickcourses.github.io/genai-vllm/genai-vllm/1/index.html

== Potential Topics to Cover in the Lab

[#secure_vllm_endpoints]
=== Securing vLLM Endpoints

* Managing service accounts for other apps

[#troubleshooting]
=== Troubleshooting vLLM instances

* Where to find events/logs

[#configuration]
=== vLLM Configuration

* Sizing KV Cache for GPUs - https://redhatquickcourses.github.io/genai-vllm/genai-vllm/1/model_sizing/index.html[^]
** Configuring --max-model-length
**  KV Cache Quantization
*** --kv-cache-dtype
* vLLM configuration/optimization best practices
** --served-model-name
** --tensor-parallel-size
** --enable-expert-parallel
** --gpu-memory-utilization
** --max-num-batched-tokens
** --enable-eager
** --limit-mm-per-prompt
* Configuring tool calling
* Configuring speculative decoding
* prefill
* TTFT
* Intertoken Latency
* Accuracy vs Latency
* Int vs Floating point
* Model Architecture and GPU Architecture
* Tuning/configuring vLLM
* Performance analysis

