= {lab_name}

## Model Evaluation Workshop
Accuracy scores and leaderboard metrics can seem impressive, but production-grade AI demands more. True success comes from evaluating models in real-world scenarios, where performance, reliability, and user satisfaction are non-negotiable.

## In This Workshop
In this hands-on, example-driven lab, you’ll move beyond leaderboard metrics to explore:

* System-level benchmarking with GuideLLM
* Task-level accuracy evaluation with lm-eval-harness

By the end of this hands-on experience, you’ll know how to:

- Use GuideLLM and lm-eval harness to evaluate your model performance and accuracy

- Interpret results meaningfully across metrics like accuracy, throughput, and latency.

- Adjust tooling variables to align LLM behavior with production SLAs and expectations

## Your Lab Environment Overview
[cols="1,2"]
|===
|Component |Details

|Virtual Environment
|`~/venv`, Python 3.12

|Operating System
|Red Hat Enterprise Linux (RHEL) 9.5, with NVIDIA drivers and CUDA toolkit installed. (CUDA is a parallel computing platform and programming model developed by NVIDIA for general computing on its own GPUs.)

|GPU
|NVIDIA L4 (24GB VRAM)

|LLM Runtime (Development)
|vLLM as our high-performance opinionated inference engine
|===


## System Preparation

### Terminal Setup

All lab activities will use the **Terminals** tab. You're already connected to your RHEL system through two tmux-enabled terminal panes. These sessions share the same environment.

### Step 1: Install NVIDIA Container Toolkit

[source,console,role=execute,subs=attributes+]
----
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
sudo dnf config-manager --enable nvidia-container-toolkit-experimental
sudo dnf install -y nvidia-container-toolkit podman
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
----