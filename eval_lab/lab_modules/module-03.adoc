:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

# Evaluating Model Accuracy with Trusty AI lm-eval-harness service

While performance metrics like latency and throughput are critical for deploying efficient GenAI systems, **task-level accuracy and reasoning quality** are equally essential for selecting or fine-tuning a model. In this activity, we use the popular lm-eval-harness framework to evaluate how well a language model performs across established benchmarks, focusing on reasoning and subject matter understanding.

## What is Trusty AI?

https://trustyai.org/docs/main/main

## What is lm-eval-harness?

**lm-eval-harness** is a community-maintained benchmarking toolkit from **EleutherAI**. It enables consistent, reproducible evaluation of large language models (LLMs) across dozens of academic and real-world benchmarks, such as:

* MMLU (Massive Multitask Language Understanding)

* HellaSwag, ARC, and Winogrande

* Question answering, common sense reasoning, reading comprehension, and more

The framework supports both open-source models and OpenAI-compatible endpoints, and can be customized with additional tasks, prompt templates, and evaluation metrics.

## MMLU-Pro

Today we will be running the mmlu_pro evaluation. 

**MMLU-Pro** is a reasoning-focused, multiple-choice benchmark derived from the original MMLU dataset. MMLU-Pro extends the original MMLU benchmark by introducing 10-option multiple-choice questions across diverse academic disciplines. It’s designed to test a model’s **reasoning, factual recall, and elimination skills**—key for enterprise AI.

## Today's Activity

In this section of our lab we will:

. Set up the Trusty AI operator
. Create and run the lm-eval job
. Interpret and understand results

### Setup Trusty AI

* Go to the DataScienceCluster resource instance.

* Change the trustyai managementState from "Removed" to "Managed"
https://github.com/trustyai-explainability/llama-stack-provider-lmeval/blob/main/demos/00-getting_started_with_lmeval.ipynb
https://trustyai.org/docs/main/lm-eval-tutorial#_examples


#### Create and run lm-eval job

Create an lm-eval job from the lmevaljob.yaml file. Let's review the contents:

[source,console,role=execute,subs=attributes+]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob
spec:
  model: local-completions
  taskList:
    taskNames:
      - mmlu
  logSamples: true
  batchSize: '1'
  # allowOnline: true
  # allowCodeExecution: true
  modelArgs:
    - name: model
      value: granite-8b-instruct-vllm-kserve
    - name: base_url
      value: https://granite-8b-instruct-vllm-kserve-guidellm-benchmark.apps.cluster-ld95z.ld95z.sandbox671.opentlc.com/v1/completions 
    - name: num_concurrent
      value:  "1"
    - name: max_retries
      value:  "3"
    - name: tokenized_requests
      value: "False"
    - name: processor
      value: ibm-granite/granite-3.3-8b-instruct
  outputs:
    pvcName: "tokenizer-pvc"
  pod: 
    container:
      resources:
          limits: 
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: '1'
----

Once the pod is created from the job, add the following to the tolerations section of the pod spec.

[source,console,role=execute,subs=attributes+]
----
tolerations:
- key: "nvidia.com/gpu"
  operator: "Equal"
  value: "True"
  effect: "NoSchedule"
----

### While You Wait: What is lm-eval-harness?

* Check out this https://github.com/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb[overview notebook] to explore extensibility and task definitions.

* View real Red Hat https://huggingface.co/collections/RedHatAI/red-hat-ai-validated-models-v10-682613dc19c4a596dbac9437[validated model results] to understand benchmark outcomes in production contexts and to see how your favorite models rank.

## Interpreting MMLU-Pro Results

**Accuracy**: The primary metric is multiple-choice accuracy, indicating how often the model selects the correct answer from 10 options.

* ~10% = random guessing baseline

* ~30–50% = typical for smaller or untuned models

* ~60–70%+ = high reasoning capability or fine-tuned performance

**Per-subject Scores**: Breakdowns by subject (e.g., philosophy, law, computer science) help identify a model’s strengths and weaknesses in specific domains.

**Implications**: Higher MMLU-Pro accuracy generally correlates with better real-world task generalization, especially for tasks involving structured inputs, knowledge retrieval, and logic.

## Summary

This activity showed how to:

Next steps might include:

